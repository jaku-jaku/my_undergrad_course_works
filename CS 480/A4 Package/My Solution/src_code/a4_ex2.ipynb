{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('anaconda3': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "95f5e6bcab80aaa5b63a63ca7d4788b69e7d528d1b5d39c579840ad9c91453be"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, List, Dict\n",
    "from enum import Enum\n",
    "from icecream import ic\n",
    "import copy\n",
    "\n",
    "import time\n",
    "\n",
    "import a4_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=== Loading Data ... \n> Augmentation: ['FLATTEN', 'NORMALIZE']\n=== Data Loaded [Testing] ===\n=== Loading Data ... \n> Augmentation: ['FLATTEN', 'NORMALIZE']\n=== Data Loaded [Training] ===\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset:\n",
    "train_dataset = a4_lib.A4_EX1_CNN_HELPER.load_mnist_data(\n",
    "    n_workers    = 1,\n",
    "    augmentation = [\"FLATTEN\", \"NORMALIZE\"],\n",
    "    shuffle      = True,\n",
    "    train_set    = True,\n",
    ")\n",
    "test_dataset  = a4_lib.A4_EX1_CNN_HELPER.load_mnist_data(\n",
    "    n_workers    = 1,\n",
    "    augmentation = [\"FLATTEN\", \"NORMALIZE\"],\n",
    "    shuffle      = False,\n",
    "    train_set    = False,\n",
    ")\n",
    "\n",
    "train_X = next(iter(train_dataset))[0].numpy()\n",
    "train_y = next(iter(train_dataset))[1].numpy()\n",
    "test_X = next(iter(test_dataset))[0].numpy()\n",
    "test_y = next(iter(test_dataset))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Feature Reduction\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pca = PCA(20)\n",
    "train_X_reduced = pca.fit_transform(train_X) \n",
    "test_X_reduced = pca.transform(test_X) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ic| np.shape(train_X): (60000, 784)\n",
      "ic| np.shape(train_y): (60000,)\n",
      "ic| np.shape(test_X): (10000, 784)\n",
      "ic| np.shape(test_y): (10000,)\n"
     ]
    }
   ],
   "source": [
    "ic(np.shape(train_X))\n",
    "ic(np.shape(train_y))\n",
    "ic(np.shape(test_X))\n",
    "ic(np.shape(test_y))\n",
    "\n",
    "TRAINING_SET = {}\n",
    "TESTING_SET = {}\n",
    "# Split training dataset based on labels:\n",
    "for i in range(10):\n",
    "    TRAINING_SET[i] = train_X_reduced[train_y == i]\n",
    "    TESTING_SET[i] = test_X_reduced[test_y == i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ic| np.shape(TRAINING_SET[i]): (5923, 20)\n",
      "ic| np.shape(TRAINING_SET[i]): (6742, 20)\n",
      "ic| np.shape(TRAINING_SET[i]): (5958, 20)\n",
      "ic| np.shape(TRAINING_SET[i]): (6131, 20)\n",
      "ic| np.shape(TRAINING_SET[i]): (5842, 20)\n",
      "ic| np.shape(TRAINING_SET[i]): (5421, 20)\n",
      "ic| np.shape(TRAINING_SET[i]): (5918, 20)\n",
      "ic| np.shape(TRAINING_SET[i]): (6265, 20)\n",
      "ic| np.shape(TRAINING_SET[i]): (5851, 20)\n",
      "ic| np.shape(TRAINING_SET[i]): (5949, 20)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    ic(np.shape(TRAINING_SET[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_GMM:\n",
    "    class VERSION(Enum):\n",
    "        DIAGNOAL = \"Diagnoal\"\n",
    "        SPHERICAL = \"Spherical\"\n",
    "\n",
    "    PDF_MODELS = {\n",
    "        VERSION.DIAGNOAL: # O(K + ND)\n",
    "            lambda pi, mu, S, k, X:  \\\n",
    "                pi[k] * ( (np.product(S[k]) * (2 * np.pi)) ** (-1/2)) * np.exp(- 0.5 * np.sum(np.divide((X - mu[k]) ** 2, S[k]), axis=1)  ),\n",
    "        VERSION.SPHERICAL: # O(ND)\n",
    "            lambda pi, mu, S, k, X:  \\\n",
    "                pi[k] * ((S[k] ** len(X[0]) * (2 * np.pi)) ** (-1/2)) * np.exp( - 0.5 / S[k] * np.diag((X - mu[k]) @ (X - mu[k]).T )),\n",
    "    }\n",
    "    S_UPDATE = {\n",
    "        VERSION.DIAGNOAL: # O(KND + K)\n",
    "            lambda R_norm, X, mu:  [ R_norm[:,k].dot((X - mu[k]) ** 2) for k in range(len(mu)) ],\n",
    "        VERSION.SPHERICAL: # O(KND)\n",
    "            lambda R_norm, X, mu:  np.sum([ R_norm[:,k].dot((X - mu[k]) ** 2) for k in range(len(mu)) ]) / len(mu),\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def fit(\n",
    "        X,\n",
    "        K: int,\n",
    "        MAX_ITR: int,\n",
    "        VER: VERSION,\n",
    "        EARLY_STOPPING_LOSS_TOL: float = 0.001,\n",
    "        verbose_level: a4_lib.VerboseLevel = a4_lib.VerboseLevel.HIGH\n",
    "    ) -> Dict[str, Any]:\n",
    "        print(\"=== Fitting [{}] :\".format(VER))\n",
    "        # const:                                (static memory):\n",
    "        N, D = np.shape(X)\n",
    "\n",
    "        # params      (run-time static memory, long-term cache):\n",
    "        pi = np.full(K, 1/K)                                    # O(K)\n",
    "        mu = np.random.rand(K, D)                               # O(K * D)\n",
    "        if VER == EM_GMM.VERSION.DIAGNOAL:\n",
    "            S = np.ones((K, D))                                 # O(K * D)\n",
    "        elif VER == EM_GMM.VERSION.SPHERICAL:\n",
    "            S = np.ones((K))                                    # O(1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid Version for EM_GMM fitting!\")\n",
    "        \n",
    "        # placeholder  (run-time heap memory, short-term cache):\n",
    "        R  = np.zeros((N, K))                                    # O(N * K)\n",
    "        R_sum_n  = np.zeros((N))                                  # O(N)\n",
    "        R_sum_k  = np.zeros((K))                                  # O(K)\n",
    "        loss = np.zeros(MAX_ITR)\n",
    "        iter_count = 0 \n",
    "        # -----> [Space Complexity]: O(N x D + K x D + N x K)\n",
    "\n",
    "        # select model:\n",
    "        # Responsibility:\n",
    "        pdf_ = EM_GMM.PDF_MODELS[VER]\n",
    "        # S update:\n",
    "        s_update_ = EM_GMM.S_UPDATE[VER]\n",
    "\n",
    "        # train:\n",
    "        for iter in range(MAX_ITR):                                     # O(MAX_ITR * (...))\n",
    "            # [Step 2] Evaluate - Expectation:\n",
    "            # Expectation:\n",
    "            for k in range(K):\n",
    "                R[:, k] = pdf_(pi=pi, mu=mu, S=S, k=k, X=X) # O(K * (O(__pdf_models__))) = O(KND)\n",
    "\n",
    "            # Normalization:\n",
    "            R_sum_n = np.sum(R, axis=1)                                 # O(NK)\n",
    "            R = np.divide(R.T, R_sum_n).T                               # O(NK)\n",
    "\n",
    "            # Negative log-likelihood (with correction):\n",
    "            loss[iter] = - np.sum(np.log(R_sum_n))                      # O(2 * N)\n",
    "\n",
    "            if verbose_level >= a4_lib.VerboseLevel.HIGH:\n",
    "                print(\" - itr:{:3d}> pi:{} mu:{} S:{} => Loss: {}\".format(iter, pi, mu, S, loss[iter]))\n",
    "            if verbose_level == a4_lib.VerboseLevel.MEDIUM:\n",
    "                print(\" - itr:{:3d}> pi:{} => Loss: {}\".format(iter, pi, loss[iter]))\n",
    "\n",
    "            # early stopping condition:\n",
    "            if (iter > 1) and (np.abs(loss[iter] - loss[iter-1]) <= np.abs(loss[iter]) * EARLY_STOPPING_LOSS_TOL):\n",
    "                break\n",
    "\n",
    "            # [Step 1] Update - Maximization:\n",
    "            # Normalization:\n",
    "            R_sum_k = np.sum(R, axis=0)                                 # O(NK)\n",
    "            R = np.divide(R, R_sum_k)                                   # O(NK)\n",
    "            # Update:\n",
    "            pi = R_sum_k / N                                            # O(K)\n",
    "            mu = np.dot(R.T, X)                                         # O(<(KN), (ND)>) = O(KND)\n",
    "            S  = s_update_(R_norm=R, X=X, mu=mu)                        # O(__s_update__)\n",
    "\n",
    "            # print(\"     itr:{:3d}> Updated: pi:{} mu:{} S:{} \".format(iter, pi, mu, S))\n",
    "            iter_count += 1\n",
    "        # -----> [Time Complexity]: O(MAX_ITR x K x N x D)\n",
    "\n",
    "        # pack parameters into a dictionary => model:\n",
    "        return {\n",
    "            \"Ver.\"  : VER,\n",
    "            # fit params:\n",
    "            \"pi\"    : pi,\n",
    "            \"mu\"    : mu,\n",
    "            \"S\"     : S,\n",
    "            # fit model status:\n",
    "            \"K\"             : K,\n",
    "            \"iter_count\"    : iter_count,\n",
    "            \"MAX_ITR\"       : EARLY_STOPPING_LOSS_TOL,\n",
    "            \"ESTOP_LOSS_TOL\": EARLY_STOPPING_LOSS_TOL,\n",
    "        }\n",
    "    \n",
    "    def predict(\n",
    "        X,\n",
    "        model\n",
    "    ):\n",
    "        pdf_ = EM_GMM.PDF_MODELS[model[\"Ver.\"]]\n",
    "        N, D = np.shape(X)\n",
    "        R  = np.zeros((N, model[\"K\"]))\n",
    "        for k in range(model[\"K\"]):\n",
    "            R[:, k] = pdf_(pi=model[\"pi\"],mu=model[\"mu\"],S= model[\"S\"], k=k, X=X) \n",
    "        Probability = np.dot(R, model[\"pi\"])\n",
    "        return Probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "= Fitting [VERSION.DIAGNOAL] :\n",
      " - itr:  0> pi:[0.2 0.2 0.2 0.2 0.2] => Loss: 17631.32903879596\n",
      " - itr:  1> pi:[0.22217994 0.22586526 0.175752   0.13083922 0.24536357] => Loss: 3707.5333827660693\n",
      " - itr:  2> pi:[0.22854145 0.23028609 0.17119499 0.1311431  0.23883438] => Loss: 3682.7066394727594\n",
      " - itr:  3> pi:[0.24028672 0.22992826 0.16838285 0.12829961 0.23310255] => Loss: 3662.065630036023\n",
      " - itr:  4> pi:[0.25369746 0.23105428 0.16993218 0.12765408 0.21766199] => Loss: 3646.1307756301335\n",
      " - itr:  5> pi:[0.26489997 0.23118074 0.17095323 0.12660068 0.20636539] => Loss: 3640.956321800594\n",
      " - itr:  6> pi:[0.27185966 0.2279557  0.17098427 0.12913576 0.2000646 ] => Loss: 3637.039741769289\n",
      " - itr:  7> pi:[0.27993847 0.2239817  0.16892868 0.13008876 0.19706239] => Loss: 3634.3003610443866\n",
      " - itr:  8> pi:[0.28835825 0.2206566  0.16257035 0.13123071 0.19718409] => Loss: 3631.8032877371224\n",
      " - itr:  9> pi:[0.29416535 0.21749894 0.15279393 0.13626842 0.19927336] => Loss: 3628.8950187803384\n",
      " - itr: 10> pi:[0.29712427 0.21226826 0.14897413 0.13730332 0.20433003] => Loss: 3627.459625662479\n",
      " - itr: 11> pi:[0.29865121 0.20479337 0.14856373 0.1363818  0.21160988] => Loss: 3625.897008610803\n",
      " - itr: 12> pi:[0.29966861 0.19926044 0.15027265 0.13431031 0.216488  ] => Loss: 3625.086951227224\n",
      " - itr: 13> pi:[0.30053197 0.19676714 0.15174528 0.13277996 0.21817565] => Loss: 3624.8867252427594\n",
      " - itr: 14> pi:[0.30125627 0.19540211 0.1521912  0.13226745 0.21888296] => Loss: 3624.8357427228484\n",
      " - itr: 15> pi:[0.30179641 0.19433673 0.15228966 0.13217011 0.21940708] => Loss: 3624.7986549533716\n",
      " - itr: 16> pi:[0.30220291 0.19320264 0.15232674 0.13219557 0.22007214] => Loss: 3624.7413115075733\n",
      " - itr: 17> pi:[0.3025428  0.19162201 0.15237704 0.13225769 0.22120046] => Loss: 3624.598513397307\n",
      " - itr: 18> pi:[0.30289972 0.1889687  0.15247299 0.13233819 0.22332039] => Loss: 3624.1951474877737\n",
      " - itr: 19> pi:[0.30345454 0.18499616 0.15265714 0.13244498 0.22644718] => Loss: 3623.6378496506427\n",
      " - itr: 20> pi:[0.30461188 0.18128305 0.15295381 0.13258515 0.22856613] => Loss: 3623.3696927271353\n",
      " - itr: 21> pi:[0.30652918 0.1776535  0.15319975 0.13269568 0.22992189] => Loss: 3623.0705621301177\n",
      " - itr: 22> pi:[0.30887152 0.17366661 0.15328088 0.1327538  0.23142719] => Loss: 3622.6878137060517\n",
      " - itr: 23> pi:[0.31073222 0.1698138  0.15317526 0.1327991  0.23347962] => Loss: 3622.3508205560875\n",
      " - itr: 24> pi:[0.31184677 0.16604784 0.15280727 0.13288355 0.23641456] => Loss: 3621.973602813785\n",
      " - itr: 25> pi:[0.3127398  0.16213843 0.15209127 0.13304153 0.23998897] => Loss: 3621.4106003225843\n",
      " - itr: 26> pi:[0.31369625 0.15824788 0.15100545 0.1333057  0.24374472] => Loss: 3620.506274771343\n",
      " - itr: 27> pi:[0.31340228 0.15560388 0.14954684 0.13377179 0.2476752 ] => Loss: 3619.702947834796\n",
      " - itr: 28> pi:[0.31228481 0.15333587 0.14769383 0.13478168 0.25190382] => Loss: 3619.114937263927\n",
      " - itr: 29> pi:[0.31197678 0.15044706 0.14477755 0.13718014 0.25561847] => Loss: 3618.200875311302\n",
      " - itr: 30> pi:[0.31191698 0.14852511 0.14019884 0.14152388 0.25783519] => Loss: 3616.9378458637407\n",
      " - itr: 31> pi:[0.31245185 0.14764906 0.13668229 0.14437604 0.25884077] => Loss: 3616.445433269824\n",
      " - itr: 32> pi:[0.31407283 0.14725074 0.13245904 0.14693998 0.25927742] => Loss: 3615.7279150814297\n",
      " - itr: 33> pi:[0.31593869 0.14702999 0.127679   0.149841   0.25951132] => Loss: 3615.033023917859\n",
      " - itr: 34> pi:[0.31665549 0.14688357 0.12468114 0.15200251 0.2597773 ] => Loss: 3614.8206472598554\n",
      " - itr: 35> pi:[0.31652687 0.14676561 0.1231505  0.15352955 0.26002748] => Loss: 3614.761410211939\n",
      " - itr: 36> pi:[0.31621108 0.14666554 0.12200106 0.15487728 0.26024504] => Loss: 3614.715694486788\n",
      " - itr: 37> pi:[0.31599058 0.14658161 0.12076961 0.15620329 0.26045491] => Loss: 3614.6608707652153\n",
      " - itr: 38> pi:[0.31606184 0.1465094  0.11914536 0.15761042 0.26067298] => Loss: 3614.5682015190114\n",
      " - itr: 39> pi:[0.31668092 0.14644438 0.11665836 0.15929712 0.26091921] => Loss: 3614.340576952269\n",
      " - itr: 40> pi:[0.31813226 0.14638195 0.11242863 0.1618278  0.26122936] => Loss: 3613.600476502703\n",
      " - itr: 41> pi:[0.32007275 0.14631601 0.10574182 0.16620905 0.26166036] => Loss: 3611.895193026654\n",
      " - itr: 42> pi:[0.32144887 0.14623912 0.10045817 0.16964107 0.26221278] => Loss: 3610.9012056039214\n",
      " - itr: 43> pi:[0.32232162 0.14615724 0.09682035 0.17212678 0.262574  ] => Loss: 3610.163320709335\n",
      " - itr: 44> pi:[0.32128595 0.14608547 0.09606579 0.17386158 0.26270121] => Loss: 3610.0351309513812\n",
      " - itr: 45> pi:[0.31948409 0.14602859 0.09660028 0.17514039 0.26274665] => Loss: 3609.995970733256\n",
      " - itr: 46> pi:[0.31780844 0.14598265 0.09714008 0.17629746 0.26277137] => Loss: 3609.968139432906\n",
      " - itr: 47> pi:[0.31642602 0.14594513 0.09756207 0.17727373 0.26279304] => Loss: 3609.951179912842\n",
      " - itr: 48> pi:[0.31537789 0.14591402 0.09786138 0.17803041 0.26281631] => Loss: 3609.942268414319\n",
      " - itr: 49> pi:[0.31463368 0.14588773 0.09805974 0.17857722 0.26284163] => Loss: 3609.9380914298094\n",
      " - itr: 50> pi:[0.31412954 0.14586519 0.09818539 0.17895216 0.26286771] => Loss: 3609.936282163634\n",
      " - itr: 51> pi:[0.31379864 0.14584572 0.09826279 0.17919992 0.26289294] => Loss: 3609.9355359795277\n",
      " - itr: 52> pi:[0.31358575 0.14582884 0.09830968 0.17935959 0.26291615] => Loss: 3609.9352359788754\n",
      " - itr: 53> pi:[0.31345039 0.14581421 0.09833782 0.17946085 0.26293673] => Loss: 3609.9351160827737\n",
      " - itr: 54> pi:[0.31336486 0.14580153 0.09835462 0.17952442 0.26295457] => Loss: 3609.935067542762\n",
      " - itr: 55> pi:[0.31331093 0.14579058 0.09836461 0.17956409 0.26296979] => Loss: 3609.9350472042292\n",
      " - itr: 56> pi:[0.31327688 0.14578114 0.09837054 0.17958877 0.26298267] => Loss: 3609.9350381615695\n",
      " - itr: 57> pi:[0.3132553  0.14577302 0.09837405 0.17960411 0.26299353] => Loss: 3609.9350337904143\n",
      " - itr: 58> pi:[0.31324153 0.14576605 0.09837612 0.17961363 0.26300267] => Loss: 3609.9350314599806\n",
      " - itr: 59> pi:[0.31323267 0.14576009 0.09837733 0.17961955 0.26301036] => Loss: 3609.9350300941887\n",
      " - itr: 60> pi:[0.3132269  0.14575498 0.09837804 0.17962324 0.26301684] => Loss: 3609.935029230228\n",
      " - itr: 61> pi:[0.31322308 0.14575063 0.09837846 0.17962553 0.2630223 ] => Loss: 3609.9350286538397\n",
      " - itr: 62> pi:[0.31322051 0.14574691 0.09837869 0.17962697 0.26302691] => Loss: 3609.9350282562123\n",
      " - itr: 63> pi:[0.31321874 0.14574375 0.09837883 0.17962787 0.26303082] => Loss: 3609.93502797642\n",
      " - itr: 64> pi:[0.3132175  0.14574105 0.0983789  0.17962843 0.26303412] => Loss: 3609.9350277772983\n",
      " - itr: 65> pi:[0.3132166  0.14573875 0.09837894 0.17962879 0.26303692] => Loss: 3609.9350276346768\n",
      " - itr: 66> pi:[0.31321593 0.1457368  0.09837896 0.17962902 0.26303929] => Loss: 3609.9350275321517\n",
      " - itr: 67> pi:[0.31321543 0.14573514 0.09837896 0.17962916 0.26304131] => Loss: 3609.935027458294\n",
      " - itr: 68> pi:[0.31321504 0.14573372 0.09837896 0.17962926 0.26304302] => Loss: 3609.9350274050234\n",
      " - itr: 69> pi:[0.31321473 0.14573252 0.09837896 0.17962932 0.26304447] => Loss: 3609.935027366571\n",
      " - itr: 70> pi:[0.31321449 0.1457315  0.09837896 0.17962936 0.2630457 ] => Loss: 3609.9350273388022\n",
      " - itr: 71> pi:[0.31321429 0.14573063 0.09837895 0.17962938 0.26304675] => Loss: 3609.9350273187424\n",
      " - itr: 72> pi:[0.31321412 0.14572989 0.09837895 0.1796294  0.26304764] => Loss: 3609.9350273042483\n",
      " - itr: 73> pi:[0.31321399 0.14572926 0.09837894 0.17962941 0.2630484 ] => Loss: 3609.9350272937727\n",
      " - itr: 74> pi:[0.31321387 0.14572872 0.09837894 0.17962942 0.26304904] => Loss: 3609.935027286202\n",
      " - itr: 75> pi:[0.31321378 0.14572827 0.09837894 0.17962943 0.26304959] => Loss: 3609.93502728073\n",
      " - itr: 76> pi:[0.3132137  0.14572788 0.09837893 0.17962943 0.26305005] => Loss: 3609.935027276774\n",
      " - itr: 77> pi:[0.31321363 0.14572755 0.09837893 0.17962944 0.26305045] => Loss: 3609.9350272739143\n",
      " - itr: 78> pi:[0.31321357 0.14572728 0.09837893 0.17962944 0.26305078] => Loss: 3609.935027271846\n",
      " - itr: 79> pi:[0.31321352 0.14572704 0.09837893 0.17962944 0.26305107] => Loss: 3609.9350272703505\n",
      " - itr: 80> pi:[0.31321348 0.14572684 0.09837892 0.17962944 0.26305131] => Loss: 3609.93502726927\n",
      " - itr: 81> pi:[0.31321345 0.14572666 0.09837892 0.17962945 0.26305152] => Loss: 3609.935027268489\n",
      " - itr: 82> pi:[0.31321342 0.14572652 0.09837892 0.17962945 0.2630517 ] => Loss: 3609.935027267924\n",
      " - itr: 83> pi:[0.31321339 0.14572639 0.09837892 0.17962945 0.26305185] => Loss: 3609.9350272675147\n",
      " - itr: 84> pi:[0.31321337 0.14572629 0.09837892 0.17962945 0.26305197] => Loss: 3609.935027267219\n",
      " - itr: 85> pi:[0.31321335 0.1457262  0.09837892 0.17962945 0.26305208] => Loss: 3609.9350272670063\n",
      " - itr: 86> pi:[0.31321334 0.14572612 0.09837892 0.17962945 0.26305217] => Loss: 3609.9350272668516\n",
      " - itr: 87> pi:[0.31321333 0.14572606 0.09837892 0.17962945 0.26305225] => Loss: 3609.93502726674\n",
      " - itr: 88> pi:[0.31321331 0.145726   0.09837892 0.17962945 0.26305232] => Loss: 3609.9350272666584\n",
      " - itr: 89> pi:[0.3132133  0.14572595 0.09837892 0.17962945 0.26305237] => Loss: 3609.9350272666\n",
      " - itr: 90> pi:[0.3132133  0.14572591 0.09837892 0.17962945 0.26305242] => Loss: 3609.935027266558\n",
      " - itr: 91> pi:[0.31321329 0.14572588 0.09837892 0.17962945 0.26305246] => Loss: 3609.9350272665274\n",
      " - itr: 92> pi:[0.31321328 0.14572585 0.09837892 0.17962945 0.2630525 ] => Loss: 3609.9350272665065\n",
      " - itr: 93> pi:[0.31321328 0.14572583 0.09837892 0.17962945 0.26305253] => Loss: 3609.9350272664897\n",
      " - itr: 94> pi:[0.31321327 0.14572581 0.09837892 0.17962945 0.26305255] => Loss: 3609.9350272664783\n",
      " - itr: 95> pi:[0.31321327 0.14572579 0.09837892 0.17962945 0.26305257] => Loss: 3609.9350272664697\n",
      " - itr: 96> pi:[0.31321327 0.14572577 0.09837892 0.17962945 0.26305259] => Loss: 3609.9350272664637\n",
      " - itr: 97> pi:[0.31321327 0.14572576 0.09837892 0.17962945 0.26305261] => Loss: 3609.935027266459\n",
      " - itr: 98> pi:[0.31321326 0.14572575 0.09837892 0.17962945 0.26305262] => Loss: 3609.935027266456\n",
      " - itr: 99> pi:[0.31321326 0.14572574 0.09837892 0.17962945 0.26305263] => Loss: 3609.9350272664533\n",
      " Summary: [8] n=100 > Elappsed: 0.08347296714782715 s  \n",
      "\n",
      "=== Fitting [VERSION.DIAGNOAL] :\n",
      " - itr:  0> pi:[0.2 0.2 0.2 0.2 0.2] => Loss: 17390.39595327205\n",
      " - itr:  1> pi:[0.13747019 0.12272593 0.29866714 0.18651748 0.25461926] => Loss: 3685.296709997121\n",
      " - itr:  2> pi:[0.11762681 0.13842681 0.29260348 0.18481224 0.26653066] => Loss: 3656.079211154234\n",
      " - itr:  3> pi:[0.11245583 0.13977327 0.3018556  0.18817234 0.25774297] => Loss: 3634.483881504886\n",
      " - itr:  4> pi:[0.11345087 0.14292801 0.30935456 0.19185634 0.24241022] => Loss: 3617.6783472937323\n",
      " - itr:  5> pi:[0.12020411 0.15352204 0.30950717 0.18854754 0.22821914] => Loss: 3601.5261419736726\n",
      " - itr:  6> pi:[0.12786913 0.1643977  0.3043648  0.1875422  0.21582617] => Loss: 3592.52768090178\n",
      " - itr:  7> pi:[0.13452833 0.17117461 0.302842   0.18854192 0.20291314] => Loss: 3583.7498705797475\n",
      " - itr:  8> pi:[0.13679362 0.17571567 0.30815826 0.18858568 0.19074676] => Loss: 3571.516889685314\n",
      " - itr:  9> pi:[0.13681619 0.17750222 0.31245763 0.19289612 0.18032783] => Loss: 3566.07496572373\n",
      " - itr: 10> pi:[0.13617651 0.17749398 0.31562429 0.19823979 0.17246543] => Loss: 3564.4618601962534\n",
      " - itr: 11> pi:[0.13413717 0.17998683 0.31861542 0.19999169 0.16726888] => Loss: 3563.4353726635063\n",
      " - itr: 12> pi:[0.13122893 0.18463181 0.31920782 0.20039497 0.16453648] => Loss: 3562.7675766321427\n",
      " - itr: 13> pi:[0.13013725 0.18814222 0.31891816 0.20061567 0.1621867 ] => Loss: 3562.3596812367714\n",
      " - itr: 14> pi:[0.13004391 0.19181155 0.31856615 0.20094325 0.15863514] => Loss: 3561.6590676175565\n",
      " - itr: 15> pi:[0.13005745 0.19409986 0.31823692 0.20172774 0.15587802] => Loss: 3561.42740124544\n",
      " - itr: 16> pi:[0.13006416 0.19487233 0.31795283 0.20337902 0.15373166] => Loss: 3561.162687281236\n",
      " - itr: 17> pi:[0.1300396  0.19573206 0.31760795 0.20665546 0.14996493] => Loss: 3560.2633221746355\n",
      " - itr: 18> pi:[0.12997466 0.19739488 0.31713161 0.21143014 0.14406872] => Loss: 3558.6663189616156\n",
      " - itr: 19> pi:[0.12986513 0.2000468  0.3165573  0.21616644 0.13736433] => Loss: 3556.692218687548\n",
      " - itr: 20> pi:[0.12978796 0.2013584  0.31596777 0.22183209 0.13105378] => Loss: 3554.5417755372314\n",
      " - itr: 21> pi:[0.12978552 0.20234467 0.31553655 0.22249015 0.12984311] => Loss: 3554.367839047677\n",
      " - itr: 22> pi:[0.12980596 0.20296106 0.31512766 0.22230019 0.12980512] => Loss: 3554.333655891181\n",
      " - itr: 23> pi:[0.12982214 0.20316133 0.31467401 0.22254578 0.12979674] => Loss: 3554.322542056517\n",
      " - itr: 24> pi:[0.1298315  0.20319125 0.31415338 0.22303028 0.12979359] => Loss: 3554.3166849044114\n",
      " - itr: 25> pi:[0.12983643 0.20318357 0.31355077 0.2236367  0.12979254] => Loss: 3554.310910895071\n",
      " - itr: 26> pi:[0.1298398  0.20317793 0.31285112 0.22433889 0.12979226] => Loss: 3554.3033601476354\n",
      " - itr: 27> pi:[0.1298432  0.20318068 0.31202505 0.22515881 0.12979225] => Loss: 3554.2922303116984\n",
      " - itr: 28> pi:[0.12984743 0.20319085 0.3110241  0.22614526 0.12979235] => Loss: 3554.274675666587\n",
      " - itr: 29> pi:[0.12985305 0.20320768 0.30977932 0.22736742 0.12979253] => Loss: 3554.2459602240106\n",
      " - itr: 30> pi:[0.12986077 0.20323253 0.30820816 0.22890575 0.12979279] => Loss: 3554.1993456999307\n",
      " - itr: 31> pi:[0.1298716  0.20326968 0.30624859 0.23081699 0.12979314] => Loss: 3554.129706500954\n",
      " - itr: 32> pi:[0.1298868  0.20332649 0.3039404  0.23305271 0.1297936 ] => Loss: 3554.043564591568\n",
      " - itr: 33> pi:[0.12990718 0.20341109 0.3014982  0.23538937 0.12979416] => Loss: 3553.9626570262662\n",
      " - itr: 34> pi:[0.12993167 0.20352555 0.29921643 0.23753159 0.12979475] => Loss: 3553.90371516246\n",
      " - itr: 35> pi:[0.12995692 0.20366099 0.29725759 0.23932923 0.12979527] => Loss: 3553.864892468\n",
      " - itr: 36> pi:[0.12997955 0.20380331 0.29560624 0.24081523 0.12979567] => Loss: 3553.837501942328\n",
      " - itr: 37> pi:[0.12999839 0.20394203 0.29416616 0.24209747 0.12979595] => Loss: 3553.815012651683\n",
      " - itr: 38> pi:[0.13001403 0.20407328 0.29282801 0.24328853 0.12979616] => Loss: 3553.793025390846\n",
      " - itr: 39> pi:[0.13002769 0.20419882 0.29148063 0.24449656 0.12979631] => Loss: 3553.767252850676\n",
      " - itr: 40> pi:[0.13004055 0.20432473 0.28999496 0.24584334 0.12979642] => Loss: 3553.7309348160497\n",
      " - itr: 41> pi:[0.13005367 0.20446123 0.28819035 0.24749823 0.12979652] => Loss: 3553.6694572435895\n",
      " - itr: 42> pi:[0.13006805 0.20462493 0.2857775  0.24973289 0.12979663] => Loss: 3553.546315758822\n",
      " - itr: 43> pi:[0.13008468 0.20484479 0.2822921  0.25298163 0.1297968 ] => Loss: 3553.274350205871\n",
      " - itr: 44> pi:[0.13010521 0.20517313 0.27722248 0.25770199 0.12979718] => Loss: 3552.7393958281737\n",
      " - itr: 45> pi:[0.1301357  0.2056849  0.27084922 0.26353206 0.12979812] => Loss: 3552.060379436353\n",
      " - itr: 46> pi:[0.13018801 0.20638336 0.26453347 0.26909513 0.12980004] => Loss: 3551.5419630128245\n",
      " - itr: 47> pi:[0.13026346 0.20710796 0.25946863 0.27335739 0.12980255] => Loss: 3551.274393711135\n",
      " - itr: 48> pi:[0.13035222 0.20769549 0.2557823  0.27636533 0.12980467] => Loss: 3551.1400187527106\n",
      " - itr: 49> pi:[0.13044286 0.20807112 0.25284292 0.27883671 0.1298064 ] => Loss: 3551.0488436076153\n",
      " - itr: 50> pi:[0.13053378 0.20826714 0.25033276 0.28105738 0.12980895] => Loss: 3550.9830996318324\n",
      " - itr: 51> pi:[0.13062802 0.20833864 0.24819052 0.28303011 0.1298127 ] => Loss: 3550.9374071355896\n",
      " - itr: 52> pi:[0.13072806 0.20832491 0.24636209 0.28476773 0.12981721] => Loss: 3550.904151001183\n",
      " - itr: 53> pi:[0.13083799 0.208253   0.2447457  0.2863414  0.12982192] => Loss: 3550.876291496035\n",
      " - itr: 54> pi:[0.13096726 0.20814276 0.24322342 0.28783997 0.12982658] => Loss: 3550.8487762710342\n",
      " - itr: 55> pi:[0.13113561 0.20800738 0.24168092 0.28934494 0.12983114] => Loss: 3550.81697427417\n",
      " - itr: 56> pi:[0.1313856  0.20785356 0.23999571 0.29092948 0.12983565] => Loss: 3550.772893005386\n",
      " - itr: 57> pi:[0.13182561 0.20768228 0.23798331 0.29266865 0.12984014] => Loss: 3550.6896750614164\n",
      " - itr: 58> pi:[0.13280284 0.20748857 0.23520145 0.29466243 0.12984472] => Loss: 3550.4049494369606\n",
      " - itr: 59> pi:[0.1355732  0.20725527 0.23021016 0.2971116  0.12984976] => Loss: 3549.0362871524\n",
      " - itr: 60> pi:[0.13984965 0.20691016 0.22268125 0.30070138 0.12985755] => Loss: 3547.4926014973034\n",
      " - itr: 61> pi:[0.14042013 0.20629026 0.21691911 0.3065058  0.1298647 ] => Loss: 3546.6248883773587\n",
      " - itr: 62> pi:[0.14046133 0.20551713 0.21187666 0.31228855 0.12985632] => Loss: 3545.8472140433337\n",
      " - itr: 63> pi:[0.1405019  0.20477174 0.20849927 0.3163806  0.12984649] => Loss: 3545.3979815706707\n",
      " - itr: 64> pi:[0.14056456 0.20413794 0.20583878 0.31962534 0.12983338] => Loss: 3545.1025324486222\n",
      " - itr: 65> pi:[0.14065947 0.20358345 0.20435448 0.3215851  0.1298175 ] => Loss: 3545.02219343839\n",
      " - itr: 66> pi:[0.14076893 0.20309605 0.2037874  0.32254122 0.1298064 ] => Loss: 3545.0077060354433\n",
      " - itr: 67> pi:[0.1408586  0.20267839 0.20352458 0.32313815 0.12980028] => Loss: 3545.00236919504\n",
      " - itr: 68> pi:[0.14092281 0.20232572 0.2033581  0.32359649 0.12979689] => Loss: 3544.9994833141445\n",
      " - itr: 69> pi:[0.14096769 0.20202743 0.20323483 0.32397507 0.12979498] => Loss: 3544.9976866881702\n",
      " - itr: 70> pi:[0.14099956 0.20177351 0.20313726 0.32429577 0.1297939 ] => Loss: 3544.9964774147606\n",
      " - itr: 71> pi:[0.14102282 0.20155613 0.20305741 0.32457034 0.12979329] => Loss: 3544.995625417652\n",
      " - itr: 72> pi:[0.1410403  0.20136927 0.20299082 0.32480665 0.12979296] => Loss: 3544.995009281156\n",
      " - itr: 73> pi:[0.14105382 0.20120824 0.20293464 0.32501053 0.12979277] => Loss: 3544.9945571541075\n",
      " - itr: 74> pi:[0.14106453 0.20106928 0.20288688 0.32518664 0.12979267] => Loss: 3544.9942226507274\n",
      " - itr: 75> pi:[0.14107321 0.20094926 0.20284607 0.32533884 0.12979262] => Loss: 3544.9939740066397\n",
      " - itr: 76> pi:[0.14108037 0.20084556 0.20281108 0.32547041 0.12979258] => Loss: 3544.993788664483\n",
      " - itr: 77> pi:[0.14108635 0.20075593 0.202781   0.32558416 0.12979256] => Loss: 3544.9936502624237\n",
      " - itr: 78> pi:[0.14109141 0.20067845 0.20275509 0.32568251 0.12979254] => Loss: 3544.9935467882674\n",
      " - itr: 79> pi:[0.14109573 0.20061146 0.20273273 0.32576756 0.12979253] => Loss: 3544.9934693612618\n",
      " - itr: 80> pi:[0.14109944 0.20055353 0.20271342 0.3258411  0.12979251] => Loss: 3544.9934113879185\n",
      " - itr: 81> pi:[0.14110263 0.20050344 0.20269672 0.32590471 0.1297925 ] => Loss: 3544.993367959544\n",
      " - itr: 82> pi:[0.14110541 0.20046011 0.20268227 0.32595973 0.12979248] => Loss: 3544.99333541485\n",
      " - itr: 83> pi:[0.14110781 0.20042263 0.20266976 0.32600734 0.12979246] => Loss: 3544.9933110192255\n",
      " - itr: 84> pi:[0.1411099  0.20039021 0.20265892 0.32604853 0.12979245] => Loss: 3544.9932927280743\n",
      " - itr: 85> pi:[0.14111172 0.20036216 0.20264952 0.32608417 0.12979243] => Loss: 3544.9932790114954\n",
      " - itr: 86> pi:[0.14111331 0.20033788 0.20264137 0.32611502 0.12979242] => Loss: 3544.9932687240025\n",
      " - itr: 87> pi:[0.14111469 0.20031687 0.2026343  0.32614172 0.12979241] => Loss: 3544.993261007525\n",
      " - itr: 88> pi:[0.1411159  0.20029869 0.20262818 0.32616484 0.12979239] => Loss: 3544.993255219044\n",
      " - itr: 89> pi:[0.14111695 0.20028296 0.20262286 0.32618485 0.12979238] => Loss: 3544.9932508765614\n",
      " - itr: 90> pi:[0.14111787 0.20026934 0.20261825 0.32620217 0.12979238] => Loss: 3544.993247618693\n",
      " - itr: 91> pi:[0.14111867 0.20025755 0.20261425 0.32621717 0.12979237] => Loss: 3544.9932451744394\n",
      " - itr: 92> pi:[0.14111936 0.20024734 0.20261078 0.32623016 0.12979236] => Loss: 3544.9932433405515\n",
      " - itr: 93> pi:[0.14111997 0.2002385  0.20260777 0.32624141 0.12979235] => Loss: 3544.993241964578\n",
      " - itr: 94> pi:[0.14112049 0.20023085 0.20260515 0.32625115 0.12979235] => Loss: 3544.9932409321573\n",
      " - itr: 95> pi:[0.14112095 0.20022423 0.20260289 0.32625959 0.12979234] => Loss: 3544.9932401574993\n",
      " - itr: 96> pi:[0.14112135 0.20021849 0.20260092 0.3262669  0.12979234] => Loss: 3544.9932395762426\n",
      " - itr: 97> pi:[0.1411217  0.20021352 0.20259922 0.32627322 0.12979233] => Loss: 3544.993239140096\n",
      " - itr: 98> pi:[0.141122   0.20020922 0.20259774 0.3262787  0.12979233] => Loss: 3544.993238812831\n",
      " - itr: 99> pi:[0.14112227 0.2002055  0.20259646 0.32628345 0.12979233] => Loss: 3544.9932385672637\n",
      " Summary: [9] n=100 > Elappsed: 0.10479497909545898 s  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PART 3 : d-GMM -------:\n",
    "# Param:\n",
    "USER_PARAM_N_SAMPLES                = 100\n",
    "USER_PARAM_VERSION                  = EM_GMM.VERSION.DIAGNOAL#DIAGNOAL\n",
    "USER_PARAM_K                        = 5\n",
    "USER_PARAM_MAX_ITR                  = 100\n",
    "USER_PARAM_EARLY_STOPPING_LOSS_TOL  = 1e-30\n",
    "USER_PAEAM_VERBOSE                  = a4_lib.VerboseLevel.MEDIUM\n",
    "# Placeholder:\n",
    "D_GMM_MODELS = []\n",
    "\n",
    "# TRAIN:\n",
    "for d in range(10):\n",
    "    t = time.time()\n",
    "    model = EM_GMM.fit(\n",
    "        X       = TRAINING_SET[d][0:USER_PARAM_N_SAMPLES, :],\n",
    "        K       = USER_PARAM_K,\n",
    "        VER     = USER_PARAM_VERSION,\n",
    "        MAX_ITR = USER_PARAM_MAX_ITR,\n",
    "        EARLY_STOPPING_LOSS_TOL = USER_PARAM_EARLY_STOPPING_LOSS_TOL,\n",
    "        verbose_level = USER_PAEAM_VERBOSE,\n",
    "    )\n",
    "    D_GMM_MODELS.append(model)\n",
    "    # report:\n",
    "    if USER_PAEAM_VERBOSE >= a4_lib.VerboseLevel.HIGH:\n",
    "        print(\" Summary: [{}] n={} > Elappsed: {} s | MODEL: \\n {} \\n\".format(d, USER_PARAM_N_SAMPLES, time.time() - t, model))\n",
    "    elif USER_PAEAM_VERBOSE >= a4_lib.VerboseLevel.LOW:\n",
    "        print(\" Summary: [{}] n={} > Elappsed: {} s  \\n\".format(d, USER_PARAM_N_SAMPLES, time.time() - t))\n",
    "\n",
    "\n",
    "# PREDICT:\n",
    "dGMM_predict_ = lambda X, models: np.argmax([ EM_GMM.fit(X=x, model=m_) for m_ in models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]> Accuracy: 4.00 %\n[1]> Accuracy: 8.00 %\n[2]> Accuracy: 8.00 %\n[3]> Accuracy: 12.00 %\n[4]> Accuracy: 17.00 %\n[5]> Accuracy: 19.00 %\n[6]> Accuracy: 8.00 %\n[7]> Accuracy: 20.00 %\n[8]> Accuracy: 12.00 %\n[9]> Accuracy: 18.00 %\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "for d in range(10):\n",
    "    y_predx = []\n",
    "    for i in range(10):\n",
    "        y_pred = EM_GMM.predict(\n",
    "            X     = TESTING_SET[d][0:USER_PARAM_N_SAMPLES, :],\n",
    "            model = D_GMM_MODELS[i],\n",
    "        )\n",
    "        y_predx.append(y_pred)\n",
    "    accuracy = np.sum(np.argmax(y_predx, axis=0) == d)/USER_PARAM_N_SAMPLES\n",
    "    print(\"[{}]> Accuracy: {:0.2f} %\".format(d, accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 316
    }
   ],
   "source": [
    "np.diag([[0,1],[1,2]])"
   ]
  }
 ]
}